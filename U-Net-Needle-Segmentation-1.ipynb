{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Needle Segmentation and Localization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In segmentation our goal is to extract a specific object or group of objects from an image. In this case the object is needle.\n",
    "\n",
    "In this work a binary classification model based on convolutional nural Network (CNNs) is proposed for needle segmentation and localization. We need to extract the needle from the back ground of image which is a binary classification problem. To do this we need to classify each pixel on the image as either being part of a needle or not. The target values are given to us as a black and white mask.\n",
    "\n",
    "With the aim of performing segmentation I would be using the UNet architecture to classify image pixels as belongong to a needle or background. UNet is a very popular image segmentation archutecture initially designed for biomedical image processing but later adapted by practitionars from other fields as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Medical image segmentation can be formulated as a pixel_level\n",
    "classification problem which can be solved by convolutional\n",
    "neural networks\n",
    "In this work needle segmentation and localization is a binary classification problem based on CNNs.\n",
    "The deep network architecture is composed of sequential convolutional layers. At each convolutional\n",
    "layer l, the input feature map (image) is convolved by a set of K kernels to generate a new feature map. A non-linear activation function is then applied to this feature map to generate the output which is the input for the next layer.The concatenation of the feature maps at each layer provides a combination of patterns to the network, which become increasingly complex for deeper layers.\n",
    "\n",
    "Training of the CNN is usually done through several iterations of stochastic gradient\n",
    "descent (SGD), in which several samples of training data (abatch) is processed by the network. At each iteration, based on the calculated loss the network parameters (kernel weights and biases) are optimized by SGD in order to decrease the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN proposed in this paper is a 2D U_Net. U_Net for segmentation usually consist of an encoder (contracting)path and a decoder (expanding) path. The encoder path consists of several convolutional layers followed by activation functions with max-pooling layers on selected feature maps.\n",
    "The encoder path decreases the resolution of the feature maps(images) by computing the maximum of small patches of units of the feature maps. However, good resolution is critical for accurate segmentation, therefore in the decoder path, upsampling is performed to restore the initial resolution, but the feature maps are concatenated to keep the computation and memory requirements tractable.\n",
    "As a result of multiple convolutional\n",
    "layers and max-pooling operations the feature maps\n",
    "are reduced and the intermediate layers of an FCN become\n",
    "successively smaller. Therefore, following the convolutions,\n",
    "an FCN uses inverse convolutions (or backward convolutions)\n",
    "to up-sample the intermediate layers until the input resolution\n",
    "is matched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# importing all basic python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "# import warnings\n",
    "import warnings\n",
    "# filter warnings\n",
    "warnings.filterwarnings('ignore')\n",
    " \n",
    "%matplotlib inline    \n",
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the Images & Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have 2d RBG data set (2D, 3 channels) from strio vision camera and my goal is segmention of image pixle as a needle or backgrand.Each image is a matrix with values that range between 0 and 255. In this section I read imges cv2.imread from needle_seg folder one by one and put them in needle image vector. I crop the image margin and then resize the image (512,512) which is the size I chose as size of input image for network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "needle_img = []\n",
    "needle_crop = []\n",
    "\n",
    "for img in glob.glob(\"../needle_seg/*.png\"):\n",
    "    n = cv2.imread(img)\n",
    "    n = cv2.cvtColor(n,cv2.COLOR_BGR2RGB)\n",
    "    cropped = n[50:(n.shape[0]-100),200:(n.shape[1]-300)]\n",
    "    needle_img.append(n)\n",
    "    needle_crop.append(cropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(needle_crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "image_indx = random.sample(range(488), 12)\n",
    "image_indx\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,9),ncols=4, nrows=3)\n",
    "\n",
    "for row_num in range(4):\n",
    "        \n",
    "    img = ax[0][row_num].imshow(needle_img[image_indx[0+row_num]],cmap='gray')\n",
    "    \n",
    "        \n",
    "    img = ax[1][row_num].imshow(needle_img[image_indx[1+row_num]],cmap='gray')\n",
    "    \n",
    "    img = ax[2][row_num].imshow(needle_img[image_indx[2+row_num]],cmap='gray')\n",
    "                    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(15,9),ncols=4, nrows=3)\n",
    "\n",
    "for row_num in range(4):\n",
    "        \n",
    "    img = ax[0][row_num].imshow(needle_crop[image_indx[0+row_num]],cmap='gray')\n",
    "    \n",
    "        \n",
    "    img = ax[1][row_num].imshow(needle_crop[image_indx[1+row_num]],cmap='gray')\n",
    "    \n",
    "    img = ax[2][row_num].imshow(needle_crop[image_indx[2+row_num]],cmap='gray')\n",
    "                    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using deep learning technieques for image segmentation requier ground truth information as annotated images to train a modle. I annotaded needle images for deep learning  using free toole apeer annotate from this www.apeer.com/annotate to create labeled image for training some of the needle images.\n",
    "\n",
    "The mask is a 2D matrix. It contains only two values that represent either black or white.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have just read 51 images and coresponiding ground truth and consider them as a training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "cv_crop = []\n",
    "for img in glob.glob(\"../needle_crop1/*.png\"):\n",
    "    \n",
    "    # read the image using skimage.io and convert to RGB with\n",
    "    n = io.imread(img)\n",
    "    n = cv2.cvtColor(n,cv2.COLOR_BGR2RGB)\n",
    "    n = cv2.cvtColor(n,cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "     # resize the image\n",
    "    n = resize(n, (512, 512), mode='constant', preserve_range=True)\n",
    "    n = n/255\n",
    "    # use np.expand dims to add a channel axis so the shape becomes (IMG_HEIGHT, IMG_WIDTH, 1)\n",
    "    n = np.expand_dims(n, axis=-1)\n",
    "    \n",
    "    cv_crop.append(n)\n",
    "    \n",
    "cv_crop[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cv_crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "cv_mask = []\n",
    "for img in glob.glob(\"../needle_crop_mask/*.ome.tiff\"):\n",
    "    \n",
    "    # read the image using skimage\n",
    "    n = io.imread(img)\n",
    "    \n",
    "    # resize the image\n",
    "    n = resize(n, (512, 512), mode='constant', preserve_range=True)\n",
    "    #n= n*255\n",
    "    \n",
    "    # use np.expand dims to add a channel axis so the shape becomes (IMG_HEIGHT, IMG_WIDTH, 1)\n",
    "    n = np.expand_dims(n, axis=-1)\n",
    "    \n",
    "    cv_mask.append(n)\n",
    "    \n",
    "cv_mask[1].shape    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_crop1 = []\n",
    "\n",
    "for img in glob.glob(\"../Untitled Folder/*.png\"):\n",
    "    \n",
    "    # read the image using skimage\n",
    "    n = cv2.imread(img)\n",
    "    n = cv2.cvtColor(n,cv2.COLOR_BGR2RGB)\n",
    "    n = cv2.cvtColor(n,cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    n = n[50:(n.shape[0]-100),200:(n.shape[1]-300)]\n",
    "    \n",
    "      # resize the image\n",
    "    n = resize(n, (512, 512), mode='constant', preserve_range=True)\n",
    "    n = n/255\n",
    "    \n",
    "    # use np.expand dims to add a channel axis so the shape becomes (IMG_HEIGHT, IMG_WIDTH, 1)\n",
    "    n = np.expand_dims(n, axis=-1)\n",
    "        \n",
    "    cv_crop1.append(n)\n",
    "    \n",
    "cv_crop1[1].shape\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(cv_crop)\n",
    "y_train = np.array(cv_mask)\n",
    "X_test = np.array(cv_crop1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Needle and ground truth image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will have a look at some random images and its ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,7),ncols=4, nrows=2)\n",
    "\n",
    "for row_num in range(4):\n",
    "    \n",
    "    image_indx = np.random.randint(51)\n",
    "\n",
    "    img = ax[0][row_num].imshow(X_train[image_indx][:,:,0],cmap='gray')\n",
    "    ax[0][row_num].title.set_text('Needle_img')\n",
    "    fig.colorbar(img, ax=ax[0][row_num])\n",
    "\n",
    "\n",
    "    mask = ax[1][row_num].imshow(y_train[image_indx][:,:,0],cmap='gray')\n",
    "    ax[1][row_num].title.set_text('Ground_truth')\n",
    "    fig.colorbar(mask, ax=ax[1][row_num])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image and its associated mask have the same shape and both are 2D. The image has pixel values in the range 0 to 1. The mask has pixel values that are either 0 or 1. 0 is black and 1 is white. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agment1_X_train = np.copy(X_train)\n",
    "agment1_y_trian = np.copy(y_train)\n",
    "\n",
    "for i in range(0,50):\n",
    "    n = random.randint(0,400)\n",
    "    agment1_y_trian[i][n:n+20]=0\n",
    "    agment1_X_train[i][n:n+20]=1/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agment_X_train = np.copy(X_train)\n",
    "agment_y_trian = np.copy(y_train)\n",
    "\n",
    "for i in range(0,50):\n",
    "    n = random.randint(0,400)\n",
    "    agment_y_trian[i][n:]=0\n",
    "    agment_X_train[i][n:]=1/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((X_train, agment1_X_train), axis=0)\n",
    "y_train = np.concatenate((y_train, agment1_y_trian), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,7),ncols=4, nrows=2)\n",
    "\n",
    "for row_num in range(4):\n",
    "    \n",
    "    image_indx = np.random.randint(51)\n",
    "\n",
    "    img = ax[0][row_num].imshow(agment1_X_train[image_indx][:,:,0],cmap='gray')\n",
    "    ax[0][row_num].title.set_text('Needle_img')\n",
    "    fig.colorbar(img, ax=ax[0][row_num])\n",
    "\n",
    "\n",
    "    mask = ax[1][row_num].imshow( agment1_y_trian[image_indx][:,:,0],cmap='gray')\n",
    "    ax[1][row_num].title.set_text('Ground_truth')\n",
    "    fig.colorbar(mask, ax=ax[1][row_num])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,7),ncols=4, nrows=2)\n",
    "\n",
    "for row_num in range(4):\n",
    "    \n",
    "    image_indx = np.random.randint(51)\n",
    "\n",
    "    img = ax[0][row_num].imshow(agment_X_train[image_indx][:,:,0],cmap='gray')\n",
    "    ax[0][row_num].title.set_text('Needle_img')\n",
    "    fig.colorbar(img, ax=ax[0][row_num])\n",
    "\n",
    "\n",
    "    mask = ax[1][row_num].imshow(agment_y_trian[image_indx][:,:,0],cmap='gray')\n",
    "    ax[1][row_num].title.set_text('Ground_truth')\n",
    "    fig.colorbar(mask, ax=ax[1][row_num])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intersection-Over-Union (IoU, Jaccard Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the IoU is the area of overlap between the predicted segmentation and the ground truth divided by the area of union between the\n",
    "predicted segmentation and the ground truth\n",
    "For binary (two classes) or multi-class segmentation, the mean IoU of the image is calculated by taking the IoU of each class and averaging them. (It’s implemented slightly differently in code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def iou_coef(y_true, y_pred, smooth=1):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true * y_pred)\n",
    "    union = K.sum(y_true)+K.sum(y_pred)-intersection\n",
    "    iou =(intersection + smooth) / (union + smooth)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_coef_loss(y_true, y_pred):\n",
    "    return -iou_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training a deep learning algorithm for segmentation, here I use 2d U-Net arhitecture. U-Net is a convolutional neural network that was developed for biomedical image segmentation. It was designed to give good results when only a small number of training images are available. It was also designed to run fast.\n",
    "\n",
    "The network architecture consists of a contracting path (left side) and an expansive path (right side).\n",
    "The contracting path follows the typical architecture of a convolutional network. It consists of the repeated application of two 3x3 convolutions (unpadded convolutions), each followed by a rectified linear unit (ReLU) and a 2x2 max pooling operation with stride 2 for downsampling. At each downsampling step we double the number of feature channels.\n",
    "\n",
    "Every step in the expansive path consists of an upsampling of the feature map followed by a 2x2 convolution (\\up-convolution\") that halves the number of feature channels, a concatenation with the correspondingly cropped feature map from the contracting path, and two 3x3 convolutions, each followed by a ReLU.\n",
    "\n",
    "The cropping is necessary due to the loss of border pixels in every convolution. At the final layer a 1x1 convolution is used to map each 16 component feature vector to the desired number of classes. In total the network has 33 convolutional layers.\n",
    "\n",
    " \n",
    "Because we treat this as a binary classification problem, we will use the Sigmoid activation function in the last layer of the neural network. We will also use the binary_crossentropy loss function. We will train the network using the images as the input and the masks as the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT= 512\n",
    "IMG_WIDTH = 512\n",
    "IMG_CHANNELS = 1\n",
    "\n",
    "\n",
    "\n",
    "# Build U-Net model\n",
    "inputs = tf.keras.layers.Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
    "\n",
    "#  CONTRACTION PATH\n",
    "c1 = tf.keras.layers.Conv2D(16, (3, 3), activation=tf.keras.activations.elu, kernel_initializer='he_normal',\n",
    "                            padding='same')(inputs)\n",
    "c1 = tf.keras.layers.Dropout(0.1)(c1)\n",
    "c1 = tf.keras.layers.Conv2D(16, (3, 3), activation=tf.keras.activations.elu, kernel_initializer='he_normal',\n",
    "                            padding='same')(c1)\n",
    "p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "c2 = tf.keras.layers.Conv2D(32, (3, 3), activation=tf.keras.activations.elu, kernel_initializer='he_normal',\n",
    "                            padding='same')(p1)\n",
    "c2 = tf.keras.layers.Dropout(0.1)(c2)\n",
    "c2 = tf.keras.layers.Conv2D(32, (3, 3), activation=tf.keras.activations.elu, kernel_initializer='he_normal',\n",
    "                            padding='same')(c2)\n",
    "p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "c3 = tf.keras.layers.Conv2D(64, (3, 3), activation=tf.keras.activations.elu, kernel_initializer='he_normal',\n",
    "                            padding='same')(p2)\n",
    "c3 = tf.keras.layers.Dropout(0.2)(c3)\n",
    "c3 = tf.keras.layers.Conv2D(64, (3, 3), activation=tf.keras.activations.elu, kernel_initializer='he_normal',\n",
    "                            padding='same')(c3)\n",
    "p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\n",
    "\n",
    "c4 = tf.keras.layers.Conv2D(128, (3, 3), activation=tf.keras.activations.elu, kernel_initializer='he_normal',\n",
    "                            padding='same')(p3)\n",
    "c4 = tf.keras.layers.Dropout(0.2)(c4)\n",
    "c4 = tf.keras.layers.Conv2D(128, (3, 3), activation=tf.keras.activations.elu, kernel_initializer='he_normal',\n",
    "                            padding='same')(c4)\n",
    "p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\n",
    "\n",
    "c5 = tf.keras.layers.Conv2D(256, (3, 3), activation=tf.keras.activations.elu, kernel_initializer='he_normal',\n",
    "                            padding='same')(p4)\n",
    "c5 = tf.keras.layers.Dropout(0.2)(c5)\n",
    "c5 = tf.keras.layers.Conv2D(256, (3, 3), activation=tf.keras.activations.elu, kernel_initializer='he_normal',\n",
    "                            padding='same')(c5)\n",
    "p5 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c5)\n",
    "\n",
    "\n",
    "c6 = tf.keras.layers.Conv2D(512, (3, 3), activation=tf.keras.activations.elu, kernel_initializer='he_normal',\n",
    "                            padding='same')(p5)\n",
    "c6 = tf.keras.layers.Dropout(0.2)(c6)\n",
    "c6 = tf.keras.layers.Conv2D(512, (3, 3), activation=tf.keras.activations.elu, kernel_initializer='he_normal',\n",
    "                            padding='same')(c6)\n",
    "p6 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c6)\n",
    "\n",
    "\n",
    "c7 = tf.keras.layers.Conv2D(1024, (3, 3), activation=tf.keras.activations.elu, kernel_initializer='he_normal',\n",
    "                            padding='same')(p6)\n",
    "c7 = tf.keras.layers.Dropout(0.3)(c7)\n",
    "c7 = tf.keras.layers.Conv2D(1024, (3, 3), activation=tf.keras.activations.elu, kernel_initializer='he_normal',\n",
    "                            padding='same')(c7)\n",
    "\n",
    "\n",
    "# EXPANTION PATH\n",
    "\n",
    "u8 = tf.keras.layers.Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(c7)\n",
    "u8 = tf.keras.layers.concatenate([u8, c6])\n",
    "c8 = tf.keras.layers.Conv2D(512, (3, 3), activation=tf.keras.activations.elu, kernel_initializer='he_normal',\n",
    "                            padding='same')(u8)\n",
    "c8 = tf.keras.layers.Dropout(0.2)(c8)\n",
    "c8 = tf.keras.layers.Conv2D(512, (3, 3), activation=tf.keras.activations.elu, kernel_initializer='he_normal',\n",
    "                            padding='same')(c8)\n",
    "\n",
    "u9 = tf.keras.layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c8)\n",
    "u9 = tf.keras.layers.concatenate([u9, c5])\n",
    "c9 = tf.keras.layers.Conv2D(256, (3, 3), activation=tf.keras.activations.elu, kernel_initializer='he_normal',\n",
    "                            padding='same')(u9)\n",
    "c9 = tf.keras.layers.Dropout(0.2)(c9)\n",
    "c9 = tf.keras.layers.Conv2D(256, (3, 3), activation=tf.keras.activations.elu, kernel_initializer='he_normal',\n",
    "                            padding='same')(c9)\n",
    "\n",
    "u10 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c9)\n",
    "u10 = tf.keras.layers.concatenate([u10, c4])\n",
    "c10 = tf.keras.layers.Conv2D(128, (3, 3), activation=tf.keras.activations.elu, kernel_initializer='he_normal',\n",
    "                            padding='same')(u10)\n",
    "c10 = tf.keras.layers.Dropout(0.2)(c10)\n",
    "c10 = tf.keras.layers.Conv2D(128, (3, 3), activation=tf.keras.activations.elu, kernel_initializer='he_normal',\n",
    "                            padding='same')(c10)\n",
    "\n",
    "u11 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c10)\n",
    "u11 = tf.keras.layers.concatenate([u11, c3])\n",
    "c11 = tf.keras.layers.Conv2D(64, (3, 3), activation=tf.keras.activations.elu, kernel_initializer='he_normal',\n",
    "                            padding='same')(u11)\n",
    "c11 = tf.keras.layers.Dropout(0.2)(c11)\n",
    "c11 = tf.keras.layers.Conv2D(64, (3, 3), activation=tf.keras.activations.elu, kernel_initializer='he_normal',\n",
    "                            padding='same')(c11)\n",
    "\n",
    "u12 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c11)\n",
    "u12 = tf.keras.layers.concatenate([u12, c2])\n",
    "c12 = tf.keras.layers.Conv2D(32, (3, 3), activation=tf.keras.activations.elu, kernel_initializer='he_normal',\n",
    "                            padding='same')(u12)\n",
    "c12 = tf.keras.layers.Dropout(0.1)(c12)\n",
    "c12 = tf.keras.layers.Conv2D(32, (3, 3), activation=tf.keras.activations.elu, kernel_initializer='he_normal',\n",
    "                            padding='same')(c12)\n",
    "\n",
    "\n",
    "u13 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c12)\n",
    "u13 = tf.keras.layers.concatenate([u13, c1], axis=3)\n",
    "c13 = tf.keras.layers.Conv2D(16, (3, 3), activation=tf.keras.activations.elu, kernel_initializer='he_normal',\n",
    "                            padding='same')(u13)\n",
    "c13 = tf.keras.layers.Dropout(0.1)(c13)\n",
    "c13 = tf.keras.layers.Conv2D(16, (3, 3), activation=tf.keras.activations.elu, kernel_initializer='he_normal',\n",
    "                            padding='same')(c13)\n",
    "\n",
    "#Because we treat this as a binary classification problem, we will use the Sigmoid activation function\n",
    "outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c13)\n",
    "\n",
    "#We will train the network using the images as the input and the masks as the target.\n",
    "# We will also use the binary_crossentropy loss function.\n",
    "model1 = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "model1.compile(optimizer='adam', loss= [iou_coef_loss], metrics= [iou_coef])\n",
    "model1.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model1, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import visualkeras\n",
    "visualkeras.layered_view(model1 , to_file='output.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training of the proposed network, we aimed to minimize a loss function that measures the quality of the segmentation on the training examples. \n",
    "We will let Keras automatically create a 10% validation set during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_valid,y_train,y_valid = train_test_split(X_train,y_train,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=5, verbose=1,\n",
    "    mode='auto', baseline=None, restore_best_weights=True)\n",
    "# This callback will stop the training when there is no improvement in\n",
    "# the loss for three consecutive epochs.\n",
    "checkpoint_path = \"training_1/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "history = model1.fit(X_train, y_train,validation_data=(X_valid,y_valid), batch_size=16, epochs=25, callbacks=[cp_callback])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot history: MAE\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history.epoch, loss, 'r', label='Training loss')\n",
    "plt.plot(history.epoch, val_loss, 'bo', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('loss Value')\n",
    "#plt.ylim([0, 1])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Grand truth with Predicted Mask "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we made predictions on X_train and X_test using the trained model \n",
    "When Predict is run, the model will output a 2D mask. Each pixel in the predicted mask represents the probability that pixel is part of a needle. We need to convert these probabilities to binary. To do this we threshold the mask by setting all pixel values that are >= 0.5 to 1. All values < 0.5 we set to zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model1.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (y_pred >= 0.5).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now plot the images to see the model results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax) = plt.subplots(figsize=(15,10), ncols=4, nrows=3)\n",
    "\n",
    "for row_num in range(4):\n",
    "    \n",
    "    image_indx = np.random.randint(len(X_train))\n",
    "    \n",
    "    img = ax[0][row_num].imshow(X_train[image_indx][:,:,0],cmap='gray')\n",
    "    ax[0][row_num].title.set_text('Needle_img')\n",
    "    fig.colorbar(img, ax=ax[0][row_num])\n",
    "\n",
    "    mask = ax[1][row_num].imshow(y_train[image_indx][:,:,0],cmap='gray')\n",
    "    ax[1][row_num].title.set_text('Ground truth')\n",
    "    fig.colorbar(mask, ax=ax[1][row_num])\n",
    "\n",
    "    pred = ax[2][row_num].imshow(y_pred[image_indx][:,:,0],cmap='gray')\n",
    "    ax[2][row_num].title.set_text('Needle_mask')\n",
    "    fig.colorbar(pred, ax=ax[2][row_num])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above images show the randomly picked needle-images, corresponding ground truth from triand data  and predicted needle-mask by the trained UNet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_val=model1.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_val = (y_pred_val >= 0.5).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, (ax) = plt.subplots(figsize=(15,10), ncols=4, nrows=3)\n",
    "\n",
    "for row_num in range(4):\n",
    "    \n",
    "    image_indx = np.random.randint(len(X_valid))\n",
    "    \n",
    "    img = ax[0][row_num].imshow(X_valid[image_indx][:,:,0],cmap='gray')\n",
    "    ax[0][row_num].title.set_text('Needle_img')\n",
    "    fig.colorbar(img, ax=ax[0][row_num])\n",
    "\n",
    "    mask = ax[1][row_num].imshow(y_valid[image_indx][:,:,0],cmap='gray')\n",
    "    ax[1][row_num].title.set_text('Ground truth')\n",
    "    fig.colorbar(mask, ax=ax[1][row_num])\n",
    "\n",
    "    pred = ax[2][row_num].imshow(y_pred_val[image_indx][:,:,0],cmap='gray')\n",
    "    ax[2][row_num].title.set_text('Needle_mask')\n",
    "    fig.colorbar(pred, ax=ax[2][row_num])\n",
    "    \n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above images show the randomly picked needle-images, corresponding ground truth from triand data  and predicted needle-mask by the trained UNet model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_test =model1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_test = (Y_pred_test >= 0.5).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax) = plt.subplots(figsize=(15,6), ncols=4, nrows=2)\n",
    "\n",
    "for row_num in range(4):\n",
    "    \n",
    "    image_indx = np.random.randint(len(X_test))\n",
    "\n",
    "    img = ax[0][row_num].imshow(X_test[image_indx][:,:,0],cmap='gray')\n",
    "    ax[0][row_num].title.set_text('Needle_img')\n",
    "    fig.colorbar(img, ax=ax[0][row_num])\n",
    "\n",
    "\n",
    "    pred = ax[1][row_num].imshow(Y_pred_test[image_indx][:,:,0],cmap='gray')\n",
    "    ax[1][row_num].title.set_text('Needle_mask')\n",
    "    fig.colorbar(pred, ax=ax[1][row_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above images show the randomly picked needle-images from test data and predicted needle-mask by the trained UNet model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leveraging the volumetric nature of the data through the inter-slice dependence of 2D slices is a key\n",
    "factor in 3D biomedical image classification and segmentation problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
